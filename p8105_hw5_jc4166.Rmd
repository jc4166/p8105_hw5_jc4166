---
title: "p8105_hw5_jc4166"
author: "Jerri Chen"
date: "11/08/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(knitr)
library(rvest)
```

## Problem 1
```{r}
set.seed(10)

iris_with_missing = iris %>% 
  map_df(~replace(.x, sample(1:150, 20), NA)) %>%
  mutate(Species = as.character(Species))
```

Write a function to replace the missing values per the following guidelines:  
-For numeric variables, you should fill in missing values with the mean of non-missing values  
-For character variables, you should fill in missing values with "virginica"  

```{r}
missing_data = function(x) {
  output = vector(length = length(x))
  
  for (i in 1:length(x)) {
    
    if (is.numeric(x[i])) {
      mean_i = mean(x[!is.na(x)])
          if (!is.na(x[i])) {output[i] = x[i]} 
        else {output[i] = mean_i}
    }
    else if (is.character(x[i])) {
        if (!is.na(x[i])) {output[i] = x[i]} 
      else {output[i] = "virginica"}
    }
  }
  output
}

iris_with_missing =  map(iris_with_missing, missing_data) %>% 
  as_tibble()
```


## Problem 2
```{r, message=FALSE, warning=FALSE}
all_files = list.files("./data")

all_data = tibble(file_name = all_files) %>% 
  mutate(contents = map(file_name, ~read_csv(file.path("./data", .)))) %>% 
  unnest() %>% 
  mutate(subject_id = file_name) %>% 
  select(subject_id, everything()) %>% 
  separate("file_name", into = c("arm")) %>% 
  mutate(
    arm = recode(arm, 
                      "con" = "control",
                      "exp" = "experimental"),
    subject_id = substr(subject_id, 1, 6)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "observation"
  ) %>% 
  mutate(
    week = substr(week, 6, 7),
    week = as.numeric(week))
```
The original data came as separate csv files for each participant. The data were combined into a single data frame, and tidied such that each variable is a single column (study arm, week of study, and observed value). Of note, if I wanted to present the data in a table to a human reader, I would not have used pivot_longer as this format is harder to read. 

Make a spaghetti plot showing observations on each subject over time
```{r}
all_data %>% 
  ggplot(aes(x = week, y = observation, color = subject_id)) + 
  geom_path() + 
  facet_grid(~arm) +
  labs(
    title = "Observations for Participants Over Time",
    x = "Week",
    y = "Observation"
   ) + 
  theme(legend.position = "bottom")
```

Comparing the control to the experimental participants, the observations for the control group do not appear to change over time, whereas the observations for the experimnetal group increase over time.

# Problem 3
Conduct a simulation to explore power in a simple linear regression.
```{r}
set.seed(1)

sim_regression = function(beta1 = 0) {
  sim_data = tibble(
    x = rnorm(30, mean = 1, sd = 1),
    y = 2 + (beta1*x) + rnorm(30, 0, sqrt(50))
  )
  
ls_fit = lm(y ~ x, data = sim_data)
  
broom::tidy(ls_fit) %>%
  filter(term == "x") %>%
  select(estimate, p.value)
}
```

Set β1=0. Generate 10000 datasets from the model.
```{r}
sim_results = 
  rerun(100, sim_regression(beta1 = 0)) %>% 
  bind_rows()
```

Repeat the above for β1={1,2,3,4,5,6}.
```{r, eval=FALSE}
sim_results_repeat = 
  tibble(beta1 = c(1,2,3,4,5,6)) %>% 
  mutate(
    output_list = map(.x = beta1, ~rerun(100, sim_regression(beta1 = .x))),
    output_df = map(output_list, bind_rows)
  ) %>% 
  unnest(output_df) %>%
  select(-output_list)
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true value of β1 on the x axis. Describe the association between effect size and power.

To calculate the proportion of times the null was rejected, we have to count the number of times the p value < 0.05 and then divide by the total.

```{r}
null_rejected = sim_results_repeat %>%
  group_by(beta1) %>% 
  count(p.value < 0.05) %>%
  mutate(power = n/sum(n)) %>%
  janitor::clean_names() %>%
  filter(p_value_0_05 == TRUE)
```

Make a plot of the above data using ggplot:
```{r}
ggplot(null_rejected, aes(x = beta1, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) 
```
As effect size (beta 1) increases, so does power.

Make a plot showing the average estimate of β̂ 1 on the y axis and the true value of β1 on the x axis.
```{r}
avg_estimate = sim_results_repeat %>% 
  group_by(beta1) %>% 
  summarize(avg_estimate = mean(estimate))

plot_full = 
ggplot(sim_results_repeat, aes(x = beta1, y = avg_estimate)) +
  geom_point() +
  geom_line()  +
  labs(
    title = "True vs. Average Estimate of Beta 1 Values",
    x = "True Value of Beta 1",
    y = "Average Estimate of Beta 1"
  )
```

Make a second plot the average estimate of β̂ 1 only in samples for which the null was rejected on the y axis and the true value of β1 on the x axis. 
```{r}
null_rejected_only = sim_results_repeat %>% 
  filter(p.value < 0.05) %>% 
  group_by(beta1) %>% 
  summarize(avg_estimate = mean(estimate)) 

plot_null = 
  ggplot(null_rejected_only, aes(x = beta1, y = avg_estimate)) +
  geom_point() +
  geom_line() +
  labs(
    title = "True vs. Avg. Estimate of Beta 1 Values (p < 0.05 only)",
    x = "True Value of Beta 1",
    y = "Average Estimate of Beta 1"
  )
```

To better visualize the differences between the two plots I will combine them:
```{r}
comparison = 
  full_join(avg_estimate, null_rejected_only, by = "beta1") %>%
  rename(beta_null_rejected = avg_estimate.y, beta_all = avg_estimate.x)

ggplot(comparison, aes(x = beta1)) +
  geom_point(aes(y = beta_null_rejected), color = "purple") +
  geom_line(aes(y = beta_null_rejected), color = "purple") + 
  geom_point(aes(y = beta_all), color = "pink") +
  geom_line(aes(y = beta_all), color = "pink") +
  labs(
    x = "True Value of Beta 1",
    y = "Average Estimate of Beta 1"
  )
```

Looking at the two plots together, one can see that at beta1 = 1, the average estimate for samples in which the null was rejected is quite far from the average estimate in the full sample, but as we go towards beta1 = 6, the estimates are much closer. This is because as the effect size (beta1) increases, there will be a higher proportion of p < 0.05 values, approximating the sample in which all the values are included.
